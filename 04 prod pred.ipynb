{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. importing train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obs</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1\n",
       "0                    \n",
       "obs               195\n",
       "max_depth           2\n",
       "min_samples_leaf    2\n",
       "n_estimators      150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = pd.read_csv('04 model test hyper param.csv', header=None, index_col=0)\n",
    "df = pd.read_csv('04 model test data.csv')\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['IsBadBuy']\n",
    "x = df.drop(['IsBadBuy','RefId'], axis=1)\n",
    "c = []\n",
    "for i in x.columns:\n",
    "    c.append(1)\n",
    "c = tuple(c)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 10)\n",
      "auc roc  : 0.835\n",
      "f1 score : 0.681\n"
     ]
    }
   ],
   "source": [
    "max_depth1 = np.round(df0[1]['max_depth'],0).astype(int)\n",
    "min_samples_leaf1 = np.round(df0[1]['min_samples_leaf'],0).astype(int)\n",
    "n_estimators1 = np.round(df0[1]['n_estimators'],0).astype(int)\n",
    "model = RandomForestClassifier(max_depth=max_depth1, min_samples_leaf=min_samples_leaf1, n_estimators=n_estimators1, \n",
    "                               max_features='log2', monotonic_cst=c, random_state=0)\n",
    "model.fit(x,y)\n",
    "print(x.shape)\n",
    "pred2 = []\n",
    "pred1 = model.predict_proba(x)[:,1]\n",
    "a1 = roc_auc_score(y,pred1)\n",
    "print('auc roc  :',np.round(a1,3))\n",
    "\n",
    "for j in pred1:\n",
    "    if j > 0.30: pred2.append(1)\n",
    "    else: pred2.append(0)\n",
    "c1 = confusion_matrix(y,pred2)\n",
    "p = c1[1][1] / (c1[0][1]+c1[1][1])\n",
    "r = c1[1][1] / (c1[1][0]+c1[1][1])\n",
    "f1 = (2*p*r) / (p+r)\n",
    "print('f1 score :',np.round(f1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxMV0DNjRppJ"
   },
   "source": [
    "### 3. test dataset\n",
    "* removed observations where IsBadBuy is missing\n",
    "* keeping relevant columns\n",
    "* ratio of acquisition cost paid for the vehicle at time of purchase is taken\n",
    "* converting categorical to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('07 model holdout.csv')\n",
    "df = df[['RefId', 'VehBCost', 'Make', 'WheelType', 'MMRAcquisitionRetailAveragePrice', \n",
    "         'MMRAcquisitonRetailCleanPrice', 'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', \n",
    "         'VNST', 'WarrantyCost', 'VehicleAge', 'VehOdo']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MMRAcquisitionRetailAveragePrice'] = df['VehBCost'] / df['MMRAcquisitionRetailAveragePrice']\n",
    "df['MMRAcquisitionRetailAveragePrice'] = np.where(df['MMRAcquisitionRetailAveragePrice']==np.inf, np.nan, df['MMRAcquisitionRetailAveragePrice'])\n",
    "df['MMRAcquisitionRetailAveragePrice'] = df['MMRAcquisitionRetailAveragePrice'].fillna(df['MMRAcquisitionRetailAveragePrice'].mean())\n",
    "\n",
    "df['MMRAcquisitonRetailCleanPrice'] = df['VehBCost'] / df['MMRAcquisitonRetailCleanPrice']\n",
    "df['MMRAcquisitonRetailCleanPrice'] = np.where(df['MMRAcquisitonRetailCleanPrice']==np.inf, np.nan, df['MMRAcquisitonRetailCleanPrice'])\n",
    "df['MMRAcquisitonRetailCleanPrice'] = df['MMRAcquisitonRetailCleanPrice'].fillna(df['MMRAcquisitonRetailCleanPrice'].mean())\n",
    "\n",
    "df['MMRCurrentRetailAveragePrice'] = df['VehBCost'] / df['MMRCurrentRetailAveragePrice']\n",
    "df['MMRCurrentRetailAveragePrice'] = np.where(df['MMRCurrentRetailAveragePrice']==np.inf, np.nan, df['MMRCurrentRetailAveragePrice'])\n",
    "df['MMRCurrentRetailAveragePrice'] = df['MMRCurrentRetailAveragePrice'].fillna(df['MMRCurrentRetailAveragePrice'].mean())\n",
    "\n",
    "df['MMRCurrentRetailCleanPrice'] = df['VehBCost'] / df['MMRCurrentRetailCleanPrice']\n",
    "df['MMRCurrentRetailCleanPrice'] = np.where(df['MMRCurrentRetailCleanPrice']==np.inf, np.nan, df['MMRCurrentRetailCleanPrice'])\n",
    "df['MMRCurrentRetailCleanPrice'] = df['MMRCurrentRetailCleanPrice'].fillna(df['MMRCurrentRetailCleanPrice'].mean())\n",
    "\n",
    "df['WarrantyCost'] = df['WarrantyCost'] / df['VehBCost']\n",
    "df['WarrantyCost'] = np.where(df['WarrantyCost']==np.inf, np.nan, df['WarrantyCost'])\n",
    "df['WarrantyCost'] = df['WarrantyCost'].fillna(df['WarrantyCost'].mean())\n",
    "\n",
    "df['VehicleAge_decades'] = df['VehicleAge']/10\n",
    "df['VehOdo_lakhs'] = df['VehOdo']/100000\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Make'] = np.where(df['Make'].isin(['FORD','PONTIAC','SATURN']), 1, \n",
    "                      np.where(df['Make'].isin(['DODGE']), -1, 0))\n",
    "df['WheelType'] = np.where(df['WheelType'].isnull(), 1, -1)\n",
    "df['VNST'] = np.where(df['VNST']=='VA', 1, -1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['VehicleAge','VehOdo','VehBCost'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. pred\n",
    "* separating out x and y\n",
    "* adding monotonic constraints\n",
    "* identifying optimal hyper-parameters\n",
    "* performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 10)\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(['RefId'], axis=1)\n",
    "print(x.shape)\n",
    "pred2 = []\n",
    "pred1 = model.predict_proba(x)[:,1]\n",
    "\n",
    "for j in pred1:\n",
    "    if j > 0.30: pred2.append(1)\n",
    "    else: pred2.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. exporting dataset\n",
    "* exporting the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred'] = pred2\n",
    "df.to_csv('04 prod pred.csv', index=False)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
